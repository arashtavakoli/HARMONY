{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have enough physiological data?\n",
    "1. Reading all of Arash's data\n",
    "2. Multivariate KDE on Arash's Data\n",
    "3. Some functions for calculating Divergence\n",
    "4. Running divergence on Arahs's data\n",
    "5. Performing the same for Participant #13 different days data\n",
    "6. Optional visualization for distributions\n",
    "7. KDE visualization for paper\n",
    "8. Same process for participant #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Reading all of Arash's data\n",
    "\n",
    "import pandas as pd \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "hr = pd.read_csv(\"C:/Users/Arsalan/Desktop/both hands and car/raw/Smartwatch_HeartRateDatum.csv\")\n",
    "dev_dic = {\"f60691a313420a4e\":\"left\" , \"39ca51c16b9ec429\":\"right\"}\n",
    "devId_vals = hr[\"DeviceId\"].unique()\n",
    "#\n",
    "\n",
    "hr[\"Timestamp\"] = pd.to_datetime(hr[\"Timestamp\"])\n",
    "#hr = hr[hr[\"Timestamp\"]<\"2020-08-22 02:15:20+00:00\"]\n",
    "hr = hr.set_index(\"Timestamp\")\n",
    "hr = hr.groupby([\"DeviceId\"]).resample('1000L').mean()\n",
    "hr = hr.reset_index()\n",
    "hr[\"HR\"] = hr[\"HR\"].bfill().ffill()\n",
    "for val in devId_vals:\n",
    "    hr_temp = hr[hr[\"DeviceId\"] == val]\n",
    "    x=[]\n",
    "    group_labels =[]\n",
    "    for i in range(20):\n",
    "        x.append(hr_temp[\"HR\"][0:(i+1)*900])\n",
    "        group_labels.append(i)\n",
    "\n",
    "    fig = ff.create_distplot(x,group_labels,show_hist=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Multivariate KDE on Arash's Data\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_ultimate = []\n",
    "\n",
    "sensors = [\"Smartwatch_AccelerometerDatum\",\"Smartwatch_GyroscopeDatum\",\"Smartwatch_HeartRateDatum\",\n",
    "           \"Smartwatch_Light\"]#,\"Smartwatch_PPGDatum\"]\n",
    "\n",
    "hr = pd.read_csv(\"C:/Users/Arsalan/Desktop/both hands and car/raw/Smartwatch_HeartRateDatum.csv\")\n",
    "dev_dic = {\"f60691a313420a4e\":\"left\" , \"39ca51c16b9ec429\":\"right\"}\n",
    "devId_vals = hr[\"DeviceId\"].unique()\n",
    "\n",
    "dfs = [pd.read_csv((\"C:/Users/Arsalan/Desktop/both hands and car/raw/\"+sensor+'.csv'), parse_dates=['Timestamp']) for sensor in sensors]\n",
    "for id in tqdm(devId_vals):\n",
    "    if id==\"f60691a313420a4e\":\n",
    "        current_df = [df[df['DeviceId']==id] for df in dfs]\n",
    "        current_df_merged = current_df[0]\n",
    "        for df in current_df[1:]:\n",
    "            current_df_merged = current_df_merged.merge(df,on=['Timestamp', 'DeviceId'], how='outer')\n",
    "        current_df_merged = current_df_merged.sort_values(by = \"Timestamp\")\n",
    "        current_df_merged[\"diff\"] = ((current_df_merged[\"Timestamp\"] - current_df_merged[\"Timestamp\"].shift()).fillna(pd.Timedelta(seconds=0)))/np.timedelta64(1, 's')\n",
    "        current_df_merged[\"group\"] = current_df_merged[\"diff\"].ge(5).cumsum()\n",
    "        current_df_merged = current_df_merged.set_index(\"Timestamp\")\n",
    "        current_df_merged = current_df_merged.groupby(['DeviceId',\"group\"]).resample('2000L').mean()\n",
    "        #print(current_df_merged)\n",
    "        df_ultimate.append(current_df_merged)\n",
    "df_ultimate=pd.concat(df_ultimate, axis=0)\n",
    "df_ultimate.drop(columns=[\"group\"],inplace=True)\n",
    "df_ultimate = df_ultimate.reset_index()\n",
    "df_ultimate = df_ultimate.bfill()\n",
    "df_ultimate.dropna(how=\"any\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Some functions for calculating divergence\n",
    "#functions for calculating the divergence\n",
    "\n",
    "def compute_probs(data, n=10): \n",
    "    h, e = np.histogram(data, n)\n",
    "    p = h/data.shape[0]\n",
    "    return e, p\n",
    "\n",
    "def support_intersection(p, q): \n",
    "    sup_int = list(\n",
    "                filter(\n",
    "                    lambda x: (x[0]!=0) & (x[1]!=0), zip(p, q)\n",
    "                )\n",
    "    )\n",
    "    return sup_int\n",
    "\n",
    "def get_probs(list_of_tuples): \n",
    "    p = np.array([p[0] for p in list_of_tuples])\n",
    "    q = np.array([p[1] for p in list_of_tuples])\n",
    "    return p, q\n",
    "\n",
    "def kl_divergence(p, q): \n",
    "    return np.sum(p*np.log(p/q))\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = (1./2.)*(p + q)\n",
    "    return (1./2.)*kl_divergence(p, m) + (1./2.)*kl_divergence(q, m)\n",
    "\n",
    "def compute_kl_divergence(train_sample, test_sample, n_bins=10): \n",
    "    \"\"\"\n",
    "    Computes the KL Divergence using the support \n",
    "    intersection between two different samples\n",
    "    \"\"\"\n",
    "    e, p = compute_probs(train_sample, n=n_bins)\n",
    "    _, q = compute_probs(test_sample, n=e)\n",
    "\n",
    "    list_of_tuples = support_intersection(p, q)\n",
    "    p, q = get_probs(list_of_tuples)\n",
    "    \n",
    "    return kl_divergence(p, q)\n",
    "\n",
    "def compute_js_divergence(train_sample, test_sample, n_bins=10): \n",
    "    \"\"\"\n",
    "    Computes the JS Divergence using the support \n",
    "    intersection between two different samples\n",
    "    \"\"\"\n",
    "    e, p = compute_probs(train_sample, n=n_bins)\n",
    "    _, q = compute_probs(test_sample, n=e)\n",
    "    \n",
    "    list_of_tuples = support_intersection(p,q)\n",
    "    p, q = get_probs(list_of_tuples)\n",
    "    \n",
    "    return js_divergence(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Running divergence on Arahs's data\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "df_cleaned = df_ultimate[df_ultimate[\"group\"] == 2]\n",
    "X = df_cleaned.drop(columns=[\"DeviceId\",\"group\",\"Timestamp\",\"diff\"]).values\n",
    "slice = 500\n",
    "for i in range(5):\n",
    "    temp_x_1 = X[0:(i+1) *(slice),:]\n",
    "    temp_x_2 = X[0:(i+2) *slice,:]\n",
    "    kde_1 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_1)\n",
    "    kde_2 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_2)\n",
    "    est_1 = kde_1.score_samples(temp_x_1)\n",
    "    est_2 = kde_1.score_samples(temp_x_2)\n",
    "    ent = compute_kl_divergence(est_1,est_2,n_bins=10)\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Performing the same for Participant #13 different days data\n",
    "\n",
    "import pandas as pd \n",
    "import pandas as pd \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "hr = pd.read_csv(\"H:/13/final_HR.csv\")\n",
    "\n",
    "hr[\"Timestamp\"] = pd.to_datetime(hr[\"Timestamp\"])\n",
    "\n",
    "hr = hr.set_index(\"Timestamp\")\n",
    "hr = hr.groupby([\"video_name\"]).resample('1000L').mean()\n",
    "hr = hr.reset_index()\n",
    "hr[\"HR\"] = hr[\"HR\"].bfill().ffill()\n",
    "\n",
    "list_of_divs = []\n",
    "X = (hr[\"HR\"].values).reshape(-1,1)\n",
    "slice = 250\n",
    "for i in tqdm(range(147)):\n",
    "    temp_x_1 = X[0:(i+1) *(slice)]\n",
    "    temp_x_2 = X[0:(i+2) *slice]\n",
    "    kde_1 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_1)\n",
    "    kde_2 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_2)\n",
    "    est_1 = kde_1.score_samples(temp_x_1)\n",
    "    est_2 = kde_1.score_samples(temp_x_2)\n",
    "    ent = compute_kl_divergence(est_1,est_2,n_bins=10)\n",
    "    list_of_divs.append(ent)\n",
    "    #print(ent)\n",
    "dic_of_divs = {}\n",
    "for i in range(147):\n",
    "    dic_of_divs[i*250] = list_of_divs[i]\n",
    "print(dic_of_divs)\n",
    "df = pd.DataFrame(list(dic_of_divs.items()),columns = ['dp','KDE'])\n",
    "df[\"time\"] = df[\"dp\"]/3600    \n",
    "\n",
    "#df = pd.read_csv(\"D:/Google Drive/UVA_BRAIn_Lab/Arash/Papers/IEEE ACCESS - Dataset Paper/kde.csv\")\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=df[\"time\"], y=df[\"KDE\"],name= \"Three Month of HR\",line=dict(width=4,color='black')))\n",
    "fig1.update_layout(    \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b> KL divergence\",'showgrid':False,'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Time (hr)\",'showgrid':False,'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':600,\n",
    "        'font':dict(size=14),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "    }\n",
    ")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Optional visualization for distributions\n",
    "import math\n",
    "x=[]\n",
    "group_labels =[]\n",
    "for i in range(10):\n",
    "    x.append(hr[\"HR\"][0:(i+1)*3600])\n",
    "    group_labels.append(math.floor(i*12000/3600)\n",
    "                       )\n",
    "fig = ff.create_distplot(x,group_labels,show_hist=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. KDE visualization for paper\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "\n",
    "channels = ['HR']\n",
    "\n",
    "files = [\"final_hr.csv\"]\n",
    "df = pd.read_csv(files[0])\n",
    "df['day'] = df['day'].apply(str)\n",
    "df = df[df[\"type\"]==\"work to home\"]\n",
    "df = df.sort_values(by=['day', 'Timestamp'])\n",
    "#df = df.set_index('Timestamp')\n",
    "\n",
    "def time_corrector(x):\n",
    "    y = x[4:6]+\"/\"+x[6:]+\"/\"+x[0:4]\n",
    "    return y\n",
    "df['correct_day'] = df['day'].apply(time_corrector)\n",
    "df['day'] = df['correct_day']\n",
    "days = df.day.unique()\n",
    "#outside_events = df.event_outside.unique()\n",
    "\n",
    "\n",
    "\n",
    "#inside_events = inside_events[:-1]\n",
    "\n",
    "\n",
    "\n",
    "for channel in channels:\n",
    "#     plt.figure(figsize=(20,20))\n",
    "#     plt.title(channel)\n",
    "    \n",
    "    datas = []\n",
    "    labels = []\n",
    "\n",
    "    for day in days:\n",
    "        df2 = df[df['day']==day].copy()\n",
    "        #df2['day'] = df2['day'].apply('str')\n",
    "        #sns.kdeplot(df2[channel])\n",
    "        #plot_df(df2, event)\n",
    "        if df2[channel].values.shape[0]>100:\n",
    "            datas.append(np.random.choice(df2[channel].values, size=df2[channel].values.shape[0]))\n",
    "            labels.append(str(day))\n",
    "        \n",
    "    \n",
    "    fig1 = ff.create_distplot(datas, labels,show_curve=True, show_rug=False,show_hist=False)\n",
    "    #fig1.update_layout(title=channel) \n",
    "    fig1.update_layout(  \n",
    "    {    \n",
    "        'xaxis':{'title':\"<b>HR (bpm)\",'linecolor':'black'},\n",
    "        'yaxis':{'title':\"<b>Probability\",'linecolor':'black'},\n",
    "        'height':1000,\n",
    "        'width':1500,\n",
    "        'font':dict(size=34),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "    \n",
    "        \n",
    "    }\n",
    "    )\n",
    "    fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"right\",\n",
    "        ))\n",
    "    fig1.update_traces(\n",
    "    line=dict( width=10),\n",
    "    selector=dict(type=\"scatter\", mode=\"lines\"))\n",
    "\n",
    "\n",
    "    fig1.show()\n",
    "\n",
    "    #fig2.show()\n",
    "    \n",
    "    # Within people variability\n",
    "pio.write_image(fig1, 'distributions_hr.pdf', width=1500, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Same process for participant #1\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_ultimate = []\n",
    "\n",
    "sensors = [\"Smartwatch_AccelerometerDatum\",\"Smartwatch_GyroscopeDatum\",\"Smartwatch_HeartRateDatum\",\n",
    "           \"Smartwatch_Light\"]#,\"Smartwatch_PPGDatum\"]\n",
    "\n",
    "hr = pd.read_csv(\"C:/Users/Arsalan/Desktop/both hands and car/Arsalan_data/Smartwatch_HeartRateDatum.csv\")\n",
    "devId_vals = hr[\"DeviceId\"].unique()\n",
    "\n",
    "dfs = [pd.read_csv((\"C:/Users/Arsalan/Desktop/both hands and car/Arsalan_data/\"+sensor+'.csv'), parse_dates=['Timestamp']) for sensor in sensors]\n",
    "for id in tqdm(devId_vals):\n",
    "    current_df = [df[df['DeviceId']==id] for df in dfs]\n",
    "    current_df_merged = current_df[0]\n",
    "    for df in current_df[1:]:\n",
    "        current_df_merged = current_df_merged.merge(df,on=['Timestamp', 'DeviceId'], how='outer')\n",
    "    current_df_merged = current_df_merged.sort_values(by = \"Timestamp\")\n",
    "    current_df_merged[\"diff\"] = ((current_df_merged[\"Timestamp\"] - current_df_merged[\"Timestamp\"].shift()).fillna(pd.Timedelta(seconds=0)))/np.timedelta64(1, 's')\n",
    "    current_df_merged[\"group\"] = current_df_merged[\"diff\"].ge(5).cumsum()\n",
    "    current_df_merged = current_df_merged.set_index(\"Timestamp\")\n",
    "    current_df_merged = current_df_merged.groupby(['DeviceId',\"group\"]).resample('2000L').mean()\n",
    "    #print(current_df_merged)\n",
    "    df_ultimate.append(current_df_merged)\n",
    "        \n",
    "df_ultimate=pd.concat(df_ultimate, axis=0)\n",
    "df_ultimate.drop(columns=[\"group\"],inplace=True)\n",
    "df_ultimate = df_ultimate.reset_index()\n",
    "df_ultimate = df_ultimate.bfill()\n",
    "#df_ultimate.dropna(how=\"any\",inplace=True)\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "results={}\n",
    "df_cleaned = df_ultimate.copy()\n",
    "X = df_cleaned.drop(columns=[\"DeviceId\",\"group\",\"Timestamp\",\"diff\"]).values\n",
    "slice = 10000\n",
    "for i in tqdm(range(500)):\n",
    "    temp_x_1 = X[0:(i+1) *(slice),:]\n",
    "    temp_x_2 = X[0:(i+2) *slice,:]\n",
    "    kde_1 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_1)\n",
    "    kde_2 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(temp_x_2)\n",
    "    est_1 = kde_1.score_samples(temp_x_1)\n",
    "    est_2 = kde_1.score_samples(temp_x_2)\n",
    "    ent = compute_kl_divergence(est_1,est_2,n_bins=10)\n",
    "    results[i] = ent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
